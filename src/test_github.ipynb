{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "should_update=True reason=\"The pull request modifies the behavior of Google Play scraping to respect the 'num_reviews' parameter, which is a documented feature in the README. The README currently states that the scraper fetches up to 100 recent reviews, but it does not mention that the 'num_reviews' parameter is now respected. This change should be reflected in the README to ensure accuracy.\" updated_readme=\"# Company Detective\\n\\nThis project summarizes publicly available information about a company. It leverages various APIs to gather and analyze data, providing a comprehensive overview of the target company.\\n\\nLive site: https://ktrnka.github.io/company-detective\\n\\n![System diagram](system_diagram.png)\\n\\n## Features\\n\\n- Multiple information sources including Crunchbase, news articles, and company websites\\n- Utilizes AI to analyze and summarize information\\n- Configured via Airtable\\n- Google Analytics integration for tracking user interactions and site performance.\\n\\n## Prerequisites\\n\\n- Python 3.11 or higher\\n- uv (Astral's Python package installer and resolver)\\n\\n## API Keys Required\\n\\nThis project requires API keys for the following services:\\n\\n- OpenAI\\n- Reddit\\n- Google Custom Search Engine\\n- Scrapfly\\n- AWS\\n- Langsmith (Optional)\\n- Crunchbase (via Scrapfly)\\n- Airtable\\n\\nEnsure you have obtained the necessary API keys before proceeding with the setup. The project is designed to handle missing API keys gracefully, but functionality may be limited without them.\\n\\n## Installation\\n\\n1. Clone the repository:\\n   ```\\n   git clone https://github.com/ktrnka/company-detective.git\\n   cd company-detective\\n   ```\\n\\n2. Install uv (if not already installed):\\n   ```\\n   make install-uv\\n   ```\\n\\n3. Install dependencies using uv:\\n   ```\\n   make install\\n   ```\\n\\n4. Set up your API keys in a `.env` file in the project root directory.\\n\\n## Usage\\n\\nThe main commands for running the company analysis are:\\n\\n1. To refresh company data:\\n   ```\\n   make refresh-data\\n   ```\\n\\n2. To build the website with analyzed data:\\n   ```\\n   make build-website\\n   ```\\n\\n3. To perform both operations sequentially:\\n   ```\\n   make build\\n   ```\\n\\nNote: The default goal for the Makefile is set to `build`, so running `make` without arguments will execute the full build process.\\n\\n## Data Sources\\n\\n- Crunchbase: Provides detailed company information, funding data, and recent news.\\n- News Articles: Gathers recent news about the company.\\n- Company Website: Extracts information directly from the company's official website.\\n- Reddit: Collects relevant discussions and mentions of the company.\\n- Glassdoor: Offers employee reviews and sentiment analysis (with improved handling for small companies).\\n- App Reviews: \\n  - Google Play Store: Scrapes reviews and respects the `num_reviews` parameter, fetching up to 100 reviews at a time. If more reviews are requested, the scraper will prioritize the most recent ones.\\n  - Apple App Store: Scrapes reviews and downsamples to 100 if more are available, ensuring balanced representation with Google Play reviews.\\n\\n## Contributing\\n\\nContributions are welcome but first contact Keith for more information on how to contribute, as the repository isn't currently set up for open contributions.\\n\\n## Testing\\n\\nThe project includes automated tests. To run the tests, use:\\n```\\nmake test\\n```\\n\\nNote that some network-based tests may be skipped to avoid dependencies on external services during CI/CD processes.\\n\\n## Development\\n\\nTo check for dead code, you can use:\\n```\\nmake vulture\\n```\\n\\n## License\\n\\nTo be determined. Please contact the repository owner for licensing information.\\n\\n## Note\\n\\nThis project is under active development. Some features or data sources may change or be refactored. Please check for updates regularly.\"\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "from pydantic import ValidationError\n",
    "from pprint import pprint\n",
    "\n",
    "github_client = Github(auth=Auth.Token(os.environ[\"GITHUB_TOKEN\"]))\n",
    "repo = github_client.get_repo(\"ktrnka/company-detective\")\n",
    "\n",
    "model = get_model(\"github\", \"gpt-4o\")\n",
    "\n",
    "try:\n",
    "    result = review_pull_request(model, repo, repo.get_pull(44), use_base_readme=True)\n",
    "    print(result)\n",
    "except ValidationError as e:\n",
    "    pprint(e.json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": [
    "# APIStatusError: Error code: 413 - {'error': {'code': 'tokens_limit_reached', 'message': 'Request body too large for gpt-4o-mini model. Max size: 8000 tokens.', 'details': 'Request body too large for gpt-4o-mini model. Max size: 8000 tokens.'}}\n",
    "\n",
    "\n",
    "\n",
    "# o1\n",
    "# BadRequestError: Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'Model o1 is enabled only for api versions 2024-12-01-preview and later'}}\n",
    "# After updating:\n",
    "# BadRequestError: Error code: 400 - {'error': {'message': \"Unsupported value: 'messages[0].role' does not support 'system' with this model.\", 'type': 'invalid_request_error', 'param': 'messages[0].role', 'code': 'unsupported_value'}}\n",
    "\n",
    "# Llama-3.3-70B-Instruct\n",
    "# NotFoundError: NOT FOUND\n",
    "\n",
    "# Meta-Llama-3.1-70B-Instruct\n",
    "# NotFoundError: NOT FOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Should update: False\n",
      "\n",
      "Reason:\n",
      "The current README is comprehensive and up-to-date. The pull request changes are primarily focused on internal code modifications and don't introduce any new features or significant changes that would require updates to the README. The existing README already covers the project's purpose, features, prerequisites, installation instructions, and usage information adequately.\n",
      "\n",
      "Updated README:\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Should update: {result.should_update}\n",
    "\n",
    "Reason:\n",
    "{result.reason}\n",
    "\n",
    "Updated README:\n",
    "{result.updated_readme}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the readme diff\n",
    "import difflib\n",
    "\n",
    "if result.should_update:\n",
    "    diff = difflib.unified_diff(\n",
    "        repo.get_readme().decoded_content.decode(\"utf-8\").splitlines(),\n",
    "        result.updated_readme.splitlines(),\n",
    "    )\n",
    "    for line in diff:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-tOcPalp-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
