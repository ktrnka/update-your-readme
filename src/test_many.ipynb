{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import github_client\n",
    "\n",
    "def build_evaluation_suite(repo_full_name: str, state: str = 'open', limit: int = 10):\n",
    "    \"\"\"Helper to identify PRs in a repository that can be used for evaluation.\"\"\"\n",
    "    print(f\"Building evaluation suite for {repo_full_name} with PRs in state {state}\")\n",
    "    repo = github_client.get_repo(repo_full_name)\n",
    "\n",
    "    pr_numbers = []\n",
    "    \n",
    "    for i, pr in enumerate(repo.get_pulls(state=state)):\n",
    "        if pr.user.login == \"github-actions[bot]\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tPR #{pr.number}, {pr.user.login}: {pr.title}\")\n",
    "        pr_numbers.append(pr.number)\n",
    "\n",
    "        if i >= limit:\n",
    "            print(f\"\\tReached limit of {limit} PRs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\tExample config: {}\\n\".format({\n",
    "        repo_full_name: pr_numbers\n",
    "    }))\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "# build_evaluation_suite(\"locustio/locust\")\n",
    "# build_evaluation_suite(\"pandas-dev/pandas\")\n",
    "# build_evaluation_suite(\"ktrnka/update-your-readme\", state='closed', limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34782608695652173, 0.4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "from typing import NamedTuple\n",
    "\n",
    "class ReadmeDiff(NamedTuple):\n",
    "    added: int\n",
    "    removed: int\n",
    "\n",
    "def diff_readmes(original_readme: str, updated_readme: str) -> ReadmeDiff:\n",
    "    diff = difflib.unified_diff(\n",
    "        original_readme.splitlines(),\n",
    "        updated_readme.splitlines(),\n",
    "    )\n",
    "    lines_added = 0\n",
    "    lines_removed = 0\n",
    "    for line in diff:\n",
    "        if line.startswith('+') and not line.startswith('+++'):\n",
    "            lines_added += 1\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            lines_removed += 1\n",
    "    return ReadmeDiff(lines_added, lines_removed)\n",
    "\n",
    "# Test\n",
    "\n",
    "assert diff_readmes(\"Hello\\nWorld\", \"Hello\\nWorld\\nGoodbye\")\n",
    "\n",
    "\n",
    "from typing import Iterable, Hashable, List\n",
    "\n",
    "\n",
    "def iterate_ngrams(tokens: List[Hashable], n: int) -> Iterable[tuple]:\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i : i + n])\n",
    "\n",
    "\n",
    "def test_iterate_ngrams():\n",
    "    assert list(iterate_ngrams([\"a\", \"b\", \"c\", \"d\"], 2)) == [\n",
    "        (\"a\", \"b\"),\n",
    "        (\"b\", \"c\"),\n",
    "        (\"c\", \"d\"),\n",
    "    ]\n",
    "\n",
    "import re\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # NOTE: This is a very, very basic tokenizer for very basic tasks.\n",
    "    return re.split(r\"\\W+\", text)\n",
    "\n",
    "\n",
    "def test_tokenize():\n",
    "    assert tokenize(\"a b c\") == [\"a\", \"b\", \"c\"]\n",
    "    assert tokenize(\"a, b, c\") == [\"a\", \"b\", \"c\"]\n",
    "\n",
    "\n",
    "def jaccard_similarity(text_a: str, text_b: str, n: int = 4):\n",
    "    summary_ngrams = set(iterate_ngrams(tokenize(text_a), n))\n",
    "    source_ngrams = set(iterate_ngrams(tokenize(text_b), n))\n",
    "\n",
    "    intersection_size = len(summary_ngrams & source_ngrams)\n",
    "    union_size = len(summary_ngrams | source_ngrams)\n",
    "    \n",
    "    return intersection_size / union_size if union_size != 0 else 0\n",
    "\n",
    "\n",
    "example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "example_summary = \"a b c d e f g h i j k\"\n",
    "\n",
    "\n",
    "jaccard_similarity(example_summary, example_source), jaccard_similarity(example_summary, example_source, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_suite = {\n",
    "    'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "    'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "}\n",
    "\n",
    "small_test_suite = {\n",
    "    'ktrnka/update-your-readme': [41, 40],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing locustio/locust...\n",
      "\tTesting PR #2899...\n",
      "\tTesting PR #2856...\n",
      "\tTesting PR #2820...\n",
      "\tTesting PR #2786...\n",
      "Testing ktrnka/update-your-readme...\n",
      "\tTesting PR #50...\n",
      "\tTesting PR #49...\n",
      "\tTesting PR #46...\n",
      "\tTesting PR #44...\n",
      "\tTesting PR #43...\n",
      "\tTesting PR #41...\n",
      "\tTesting PR #40...\n",
      "\n",
      "Tested against 11 PRs in 2 repos.\n",
      "\n",
      "0% failed.\n",
      "Total runtime: 226s\n",
      "Mean runtime per PR: 21s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional, Tuple\n",
    "from main import ReadmeRecommendation, review_pull_request\n",
    "from time import time\n",
    "\n",
    "class SingleOutcome(NamedTuple):\n",
    "    result: Optional[ReadmeRecommendation]\n",
    "    error: Optional[ValueError]\n",
    "    seconds: float\n",
    "    diff: Optional[Tuple]\n",
    "    similarity: Optional[float]\n",
    "\n",
    "test_suite = medium_test_suite\n",
    "\n",
    "outcomes = {}\n",
    "for repo_name, pr_numbers in test_suite.items():\n",
    "    print(f\"Testing {repo_name}...\")\n",
    "    for pr_number in pr_numbers:\n",
    "        print(f\"\\tTesting PR #{pr_number}...\")\n",
    "\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        try:\n",
    "            repo = github_client.get_repo(repo_name)\n",
    "            pr = repo.get_pull(pr_number)\n",
    "\n",
    "            # Get the base README\n",
    "            base_readme = repo.get_contents(\"README.md\", ref=pr.base.sha).decoded_content.decode()\n",
    "\n",
    "            result = review_pull_request(repo, pr)\n",
    "\n",
    "            diff_results = None\n",
    "            similarity = None\n",
    "            if result.should_update:\n",
    "                diff_results = diff_readmes(base_readme, result.updated_readme)\n",
    "                similarity = jaccard_similarity(result.updated_readme, base_readme)\n",
    "\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(result, None, time() - start_time, diff_results, similarity)\n",
    "        except ValueError as e:\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(None, e, time() - start_time, None, None)\n",
    "\n",
    "# summarize the results\n",
    "percent_failed = len([outcome for outcome in outcomes.values() if outcome.result is None]) / len(outcomes)\n",
    "total_runtime = sum(outcome.seconds for outcome in outcomes.values())\n",
    "mean_runtime = total_runtime / len(outcomes)\n",
    "\n",
    "print(f\"\"\"\n",
    "Tested against {len(outcomes)} PRs in {len(test_suite)} repos.\n",
    "\n",
    "{percent_failed:.0%} failed.\n",
    "Total runtime: {total_runtime:.0f}s\n",
    "Mean runtime per PR: {mean_runtime:.0f}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review errors\n",
    "errors = [outcome for outcome in outcomes.values() if outcome.error is not None]\n",
    "\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.error:\n",
    "        print(f\"{outcome_id}: {outcome.result} in {outcome.seconds:.0f}s\")\n",
    "        errors = outcome.error.errors()\n",
    "        for error in errors:\n",
    "            print(f\"\\tError: {error['msg']}\")\n",
    "        example_error_outcome = outcome\n",
    "        # print(f\"\\tError: {outcome.error.msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic_core._pydantic_core.ValidationError\n",
    "\n",
    "# dir(outcome.error)\n",
    "# outcome.error.errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0% of PRs with should_update had no changes.\n",
      "    Lines changed: [13, 90, 18, 10, 5, 15, 3, 6, 10, 6]\n",
      "Percent of READMEs that were lengthened: 100%\n",
      "Mean similarity: 82.6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "should_updates = [outcome for outcome in outcomes.values() if outcome.result is not None and outcome.result.should_update]\n",
    "\n",
    "lines_changed = [sum(outcome.diff) for outcome in should_updates]\n",
    "similarities = [outcome.similarity for outcome in should_updates]\n",
    "\n",
    "percent_bad_diff = len([outcome for outcome in should_updates if outcome.diff == (0, 0)]) / len(should_updates)\n",
    "mean_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "percent_lengthened = len([outcome for outcome in should_updates if outcome.diff[0] > outcome.diff[1]]) / len(should_updates)\n",
    "\n",
    "print(f\"\"\"\n",
    "{percent_bad_diff:.0%} of PRs with should_update had no changes.\n",
    "    Lines changed: {lines_changed}\n",
    "Percent of READMEs that were lengthened: {percent_lengthened:.0%}\n",
    "Mean similarity: {mean_similarity:.1%}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ('locustio/locust', 2899)\n",
      "Automated review took 36s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces significant changes to the testing mechanisms, which improve the reliability and efficiency of the test suite. This is an important update that should be reflected in the README to highlight Locust's improved testing capabilities and ongoing development efforts.\n",
      "Diff: +12, -1 ✅\n",
      "Jaccard coef of 4grams: 91.0% ✅\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2856)\n",
      "Automated review took 29s\n",
      "Should update? True\n",
      "\n",
      "Reason: The existing README is already well-structured and informative, but it can be improved by incorporating some of the changes from the pull request and enhancing the clarity and organization of certain sections. The updates will make the README more user-friendly and provide a better introduction to Locust for new users.\n",
      "Diff: +70, -20 ✅\n",
      "Jaccard coef of 4grams: 32.1% ❌\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2820)\n",
      "Automated review took 6s\n",
      "Should update? False\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2786)\n",
      "Automated review took 35s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to reflect the new CLI argument for selecting UserClasses to run. This change is significant as it introduces a new way to specify user classes and deprecates the old method. Updating the README will help users understand and adopt the new usage pattern.\n",
      "Diff: +17, -1 ✅\n",
      "Jaccard coef of 4grams: 91.3% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 50)\n",
      "Automated review took 15s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to reflect the changes in the project structure and the addition of a new GitHub Actions workflow. The main.py file has been removed, and a new readme_feedback.yml workflow has been added. These changes should be documented in the README for accuracy and to keep users informed about the project's current state and capabilities.\n",
      "Diff: +7, -3 ✅\n",
      "Jaccard coef of 4grams: 90.9% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 49)\n",
      "Automated review took 14s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new error handling feature in the `repo_contents_to_markdown` function, which should be reflected in the README to inform users about this improvement in error handling.\n",
      "Diff: +5, -0 ✅\n",
      "Jaccard coef of 4grams: 90.3% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 46)\n",
      "Automated review took 20s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces changes to the GitHub Actions workflow and the action.yml file, which affect the usage and setup of the project. These changes should be reflected in the README to ensure users have the most up-to-date information on how to use the action.\n",
      "Diff: +15, -0 ✅\n",
      "Jaccard coef of 4grams: 78.9% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 44)\n",
      "Automated review took 13s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new file `main.py` in the `src` directory, which is not reflected in the current README. The project structure section of the README should be updated to include this new file.\n",
      "Diff: +2, -1 ✅\n",
      "Jaccard coef of 4grams: 98.2% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 43)\n",
      "Automated review took 19s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new feature to supply user feedback to the README update process. This significant change should be reflected in the README to inform users about this new functionality and how to use it.\n",
      "Diff: +6, -0 ✅\n",
      "Jaccard coef of 4grams: 85.8% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 41)\n",
      "Automated review took 21s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces significant changes to the core functionality of the project, particularly in error handling and validation for the README update process. These changes should be reflected in the README to provide users with up-to-date information about the project's reliability and error handling capabilities.\n",
      "Diff: +10, -0 ✅\n",
      "Jaccard coef of 4grams: 82.3% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 40)\n",
      "Automated review took 17s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new feature to supply user feedback to the README update process. This significant change should be reflected in the README to inform users about this new functionality and how to use it.\n",
      "Diff: +6, -0 ✅\n",
      "Jaccard coef of 4grams: 84.9% ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "good_icon = \"✅\"\n",
    "bad_icon = \"❌\"\n",
    "\n",
    "\n",
    "# Review non-errors\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.result:\n",
    "        print(f\"\"\"\n",
    "# {outcome_id}\n",
    "Automated review took {outcome.seconds:.0f}s\n",
    "Should update? {outcome.result.should_update}\n",
    "\"\"\")\n",
    "        if outcome.result.should_update:\n",
    "            diff_review = good_icon if sum(outcome.diff) > 0 else bad_icon\n",
    "            similarity_review = good_icon if 0.75 < outcome.similarity < 1. else bad_icon\n",
    "            print(f\"\"\"Reason: {outcome.result.reason}\n",
    "Diff: +{outcome.diff.added}, -{outcome.diff.removed} {diff_review}\n",
    "Jaccard coef of 4grams: {outcome.similarity:.1%} {similarity_review}\n",
    "\"\"\")\n",
    "\n",
    "            # if sum(outcome.diff) > 0:\n",
    "            #     print(f\"\"\"Updated README: \\n{outcome.result.updated_readme}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday afternoon\n",
    "\n",
    "I learned the hard way that something about the ChatPromptTemplate was disabling the Python variables in the Human step,\n",
    "so it was generating the readme purely from guidelines\n",
    "\n",
    "## Checking for \"addition bias\"\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "        Lines changed: [13, 90, 18, 10, 5, 15, 3, 6, 10, 6]\n",
    "    Percent of READMEs that were lengthened: 100%\n",
    "    Mean similarity: 82.6%\n",
    "\n",
    "So 100% of the suggestions here were additions\n",
    "\n",
    "## Trying Jaccard coef\n",
    "\n",
    "Using Jaccard coefficient to assess the extensiveness of the change is useful BUT I have to get used to it and calibrate towards a \"healthy\" range.\n",
    "\n",
    "Updated summary stats (Haiku):\n",
    "\n",
    "    9% of PRs with should_update had no changes.\n",
    "        Lines changed: [15, 76, 87, 14, 9, 0, 8, 3, 19, 73, 50]\n",
    "    Mean similarity: 68.4%\n",
    "\n",
    "Same thing but with Sonnet:\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "        Lines changed: [13, 90, 18, 10, 5, 15, 3, 6, 10, 6]\n",
    "    Mean similarity: 82.6%\n",
    "\n",
    "Ok that's a good sign.\n",
    "\n",
    "## Trying to see if Sonnet fixes some of the quality issues\n",
    "\n",
    "Baseline (Haiku):\n",
    "\n",
    "    20% of PRs with should_update had no changes.\n",
    "    40% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "Sonnet:\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 253s\n",
    "    Mean runtime per PR: 23s (slightly faster than the original non-cached results)\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "    9% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "The one bad case looked like it did a more extensive README rewrite\n",
    "\n",
    "\n",
    "## Increased the max_tokens to 4096 (Haiku's max)\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 105s\n",
    "    Mean runtime per PR: 10s\n",
    "\n",
    "Run 2:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 94s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Ok that did it. Now I'll move on to the next wave of issues:\n",
    "\n",
    "- should_update = True but it outputs the exact README as the input\n",
    "\n",
    "## 1 try only, and lowering temperature\n",
    "\n",
    "Temp 0\n",
    "\n",
    "    55% failed.\n",
    "    Total runtime: 97s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Oof\n",
    "\n",
    "Temp 0.3, which some summarization folks like:\n",
    "\n",
    "    45% failed.\n",
    "    Total runtime: 102s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Eh, I'm going to dial back the summarization stuff\n",
    "\n",
    "## After the fix:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    36% failed.\n",
    "    Total runtime: 91s\n",
    "    Mean runtime per PR: 8s\n",
    "\n",
    "The failed cases are doing should_update=True and updated_readme = nothing. I saw in the raw output that one of the really bad subtractions just had [rest of readme the same] or some such at the bottom.\n",
    "\n",
    "I've done some re-runs and it tends to be 27%-36% error rate which is worse than before (18% just before prompt caching)\n",
    "\n",
    "I attempted to manually specify the input variables but they're completely ignored. I also tried wrapping the human message in an array like the system message but that also failed.\n",
    "\n",
    "After some more experiments I think I figured out why the results are different than before:\n",
    "- Previously I defaulted tries_remaining to 1 but that actually meant it could try and retry\n",
    "- I set that to 1, and re-ran twice and got 27% failure and 18%, so I think that brings it closer to previous results\n",
    "\n",
    "Also I added more instrumentation on the output. This makes it easy to catch cases where it generates a README unrelated to the input repo. It also led me to find many cases in which it said should_update=True and regenerated the EXACT input README which is another red-flag to consider\n",
    "\n",
    "# Dev log: Monday\n",
    "\n",
    "Refactor:\n",
    "- More controlled experiment (multiple repos, build a fixed set of PRs ahead of time)\n",
    "- Track failure rate and execution time\n",
    "- Hold onto any objects to adhoc analysis after running\n",
    "\n",
    "Test suites:\n",
    "\n",
    "    medium_test_suite = {\n",
    "        'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "        'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    }\n",
    "\n",
    "    small_test_suite = {\n",
    "        'ktrnka/update-your-readme': [41, 40],\n",
    "    }\n",
    "\n",
    "## Medium test suite\n",
    "\n",
    "### Baseline with Haiku, before removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 135s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "### After removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 129s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "It's slightly faster but not a lot. I'll keep the change though.\n",
    "\n",
    "### Adding prompt caching\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 54s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Notes\n",
    "- It throws an annoying warning \"extra_headers was transferred to model_kwargs\" but that's what the docs show: https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html\n",
    "- The speedup is wonderful! That's what I'd hoped for\n",
    "- The 0% failure rate is surprising. It's possible that it's a result of needing to refactor to use the SystemMessage vs HumanMessage\n",
    "\n",
    "I'm going to re-run this without any changes cause I kind of don't even believe that we have no errors now:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 53s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Huh\n",
    "\n",
    "## Small test suite\n",
    "\n",
    "### Baseline test\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    50% failed.\n",
    "    Total runtime: 55s\n",
    "    Mean runtime per PR: 28s\n",
    "\n",
    "### With Claude 3 Haiku\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 22s\n",
    "    Mean runtime per PR: 11s\n",
    "\n",
    "\n",
    "\n",
    "# Dev log: Sunday\n",
    "\n",
    "## Before prompt engineering, running on Locust\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Stronger guidance in the prompt itself, like the Pydantic field descriptions and how they're mentioned in the prompt itself\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 2, 'should_update': 1, 'no_update': 1})\n",
    "\n",
    "## Retries\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Prompt updates, Pydantic model updates\n",
    "Counter({'should_update': 3, 'ValueError': 1})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-tOcPalp-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
