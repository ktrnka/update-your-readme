{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building evaluation suite for locustio/locust with PRs in state open\n",
      "\tPR #2899, fletelli42: Add polling mechanism in tests\n",
      "\tPR #2856, plaindocs: Light refactor, questions to address, and alt-text\n",
      "\tPR #2820, bakhtos: Handle task weights efficiently in `TaskSet`s\n",
      "\tPR #2786, bakhtos: CLI argument for selecting `UserClasses` to run\n",
      "\tExample config: {'locustio/locust': [2899, 2856, 2820, 2786]}\n",
      "\n",
      "Building evaluation suite for ktrnka/update-your-readme with PRs in state closed\n",
      "\tPR #50, martilar: Remove main.py\n",
      "\tPR #49, martilar: Try commiting change as contributor\n",
      "\tPR #46, ktrnka: Debugging, checkout into enlistment\n",
      "\tPR #44, martilar: Add nonsense to README\n",
      "\tPR #43, martilar: Add option to supply user-feedback\n",
      "\tPR #41, ktrnka: Making it more reliable against validation errors\n",
      "\tPR #40, martilar: Add option to supply user-feedback\n",
      "\tReached limit of 10 PRs\n",
      "\tExample config: {'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from core import github_client\n",
    "\n",
    "def build_evaluation_suite(repo_full_name: str, state: str = 'open', limit: int = 10):\n",
    "    print(f\"Building evaluation suite for {repo_full_name} with PRs in state {state}\")\n",
    "    repo = github_client.get_repo(repo_full_name)\n",
    "\n",
    "    pr_numbers = []\n",
    "    \n",
    "    for i, pr in enumerate(repo.get_pulls(state=state)):\n",
    "        if pr.user.login == \"github-actions[bot]\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tPR #{pr.number}, {pr.user.login}: {pr.title}\")\n",
    "        pr_numbers.append(pr.number)\n",
    "\n",
    "        if i >= limit:\n",
    "            print(f\"\\tReached limit of {limit} PRs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\tExample config: {}\\n\".format({\n",
    "        repo_full_name: pr_numbers\n",
    "    }))\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "build_evaluation_suite(\"locustio/locust\")\n",
    "# build_evaluation_suite(\"pandas-dev/pandas\")\n",
    "build_evaluation_suite(\"ktrnka/update-your-readme\", state='closed', limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_suite = {\n",
    "    'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "    'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "}\n",
    "\n",
    "small_test_suite = {\n",
    "    'ktrnka/update-your-readme': [41, 40],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing locustio/locust...\n",
      "\tTesting PR #2899...\n",
      "\tTesting PR #2856...\n",
      "\tTesting PR #2820...\n",
      "Validation error, trying again\n",
      "\tTesting PR #2786...\n",
      "Validation error, trying again\n",
      "Testing ktrnka/update-your-readme...\n",
      "\tTesting PR #50...\n",
      "Validation error, trying again\n",
      "\tTesting PR #49...\n",
      "\tTesting PR #46...\n",
      "\tTesting PR #44...\n",
      "\tTesting PR #43...\n",
      "\tTesting PR #41...\n",
      "\tTesting PR #40...\n",
      "\n",
      "Tested against 11 PRs in 2 repos.\n",
      "\n",
      "18% failed.\n",
      "Total runtime: 135s\n",
      "Mean runtime per PR: 12s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from core import UpdateRecommendation, review_pull_request\n",
    "from time import time\n",
    "\n",
    "class SingleOutcome(NamedTuple):\n",
    "    result: Optional[UpdateRecommendation]\n",
    "    error: Optional[ValueError]\n",
    "    seconds: float\n",
    "\n",
    "test_suite = medium_test_suite\n",
    "\n",
    "outcomes = {}\n",
    "for repo_name, pr_numbers in test_suite.items():\n",
    "    print(f\"Testing {repo_name}...\")\n",
    "    for pr_number in pr_numbers:\n",
    "        print(f\"\\tTesting PR #{pr_number}...\")\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        try:\n",
    "            repo = github_client.get_repo(repo_name)\n",
    "            pr = repo.get_pull(pr_number)\n",
    "            result = review_pull_request(repo, pr, tries_remaining=1)\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(result, None, time() - start_time)\n",
    "        except ValueError as e:\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(None, e, time() - start_time)\n",
    "\n",
    "# summarize the results\n",
    "percent_failed = len([outcome for outcome in outcomes.values() if outcome.result is None]) / len(outcomes)\n",
    "total_runtime = sum(outcome.seconds for outcome in outcomes.values())\n",
    "mean_runtime = total_runtime / len(outcomes)\n",
    "\n",
    "print(f\"\"\"\n",
    "Tested against {len(outcomes)} PRs in {len(test_suite)} repos.\n",
    "\n",
    "{percent_failed:.0%} failed.\n",
    "Total runtime: {total_runtime:.0f}s\n",
    "Mean runtime per PR: {mean_runtime:.0f}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # diff = difflib.unified_diff(\n",
    "            #     repo_name.get_readme().decoded_content.decode(\"utf-8\").splitlines(),\n",
    "            #     result.updated_readme.splitlines(),\n",
    "            # )\n",
    "            # for line in diff:\n",
    "            #     print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev log: Monday\n",
    "\n",
    "Refactor:\n",
    "- More controlled experiment (multiple repos, build a fixed set of PRs ahead of time)\n",
    "- Track failure rate and execution time\n",
    "- Hold onto any objects to adhoc analysis after running\n",
    "\n",
    "Test suites:\n",
    "\n",
    "    medium_test_suite = {\n",
    "        'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "        'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    }\n",
    "\n",
    "    small_test_suite = {\n",
    "        'ktrnka/update-your-readme': [41, 40],\n",
    "    }\n",
    "\n",
    "## Medium test suite\n",
    "\n",
    "### Baseline with Haiku, before removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 135s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "\n",
    "\n",
    "## Small test suite\n",
    "\n",
    "### Baseline test\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    50% failed.\n",
    "    Total runtime: 55s\n",
    "    Mean runtime per PR: 28s\n",
    "\n",
    "### With Claude 3 Haiku\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 22s\n",
    "    Mean runtime per PR: 11s\n",
    "\n",
    "\n",
    "\n",
    "# Dev log: Sunday\n",
    "\n",
    "## Before prompt engineering, running on Locust\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Stronger guidance in the prompt itself, like the Pydantic field descriptions and how they're mentioned in the prompt itself\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 2, 'should_update': 1, 'no_update': 1})\n",
    "\n",
    "## Retries\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Prompt updates, Pydantic model updates\n",
    "Counter({'should_update': 3, 'ValueError': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-LRwUm5rQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
