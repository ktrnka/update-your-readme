{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building evaluation suite for ktrnka/company-detective with PRs in state closed\n",
      "\tPR #14, ktrnka: Exclude a network-based test\n",
      "\tPR #12, ktrnka: Bring Crunchbase back\n",
      "\tPR #9, ktrnka: Dynamic rebuild cadence\n",
      "\tPR #7, ktrnka: Move the generic article scraping into utils\n",
      "\tPR #5, ktrnka: Re-organize the repo / cleanup\n",
      "\tReached limit of 10 PRs\n",
      "\tExample config: {'ktrnka/company-detective': [14, 12, 9, 7, 5]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from main import github_client\n",
    "\n",
    "def build_evaluation_suite(repo_full_name: str, state: str = 'open', limit: int = 10):\n",
    "    \"\"\"Helper to identify PRs in a repository that can be used for evaluation.\"\"\"\n",
    "    print(f\"Building evaluation suite for {repo_full_name} with PRs in state {state}\")\n",
    "    repo = github_client.get_repo(repo_full_name)\n",
    "\n",
    "    pr_numbers = []\n",
    "    \n",
    "    for i, pr in enumerate(repo.get_pulls(state=state)):\n",
    "        if pr.user.login == \"github-actions[bot]\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tPR #{pr.number}, {pr.user.login}: {pr.title}\")\n",
    "        pr_numbers.append(pr.number)\n",
    "\n",
    "        if i >= limit:\n",
    "            print(f\"\\tReached limit of {limit} PRs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\tExample config: {}\\n\".format({\n",
    "        repo_full_name: pr_numbers\n",
    "    }))\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "# build_evaluation_suite(\"locustio/locust\")\n",
    "# build_evaluation_suite(\"pandas-dev/pandas\")\n",
    "# build_evaluation_suite(\"ktrnka/update-your-readme\", state='closed', limit=10)\n",
    "\n",
    "build_evaluation_suite(\"ktrnka/company-detective\", state='closed', limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34782608695652173, 0.4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import difflib\n",
    "from typing import NamedTuple\n",
    "\n",
    "class ReadmeDiff(NamedTuple):\n",
    "    added: int\n",
    "    removed: int\n",
    "\n",
    "def diff_readmes(original_readme: str, updated_readme: str) -> ReadmeDiff:\n",
    "    diff = difflib.unified_diff(\n",
    "        original_readme.splitlines(),\n",
    "        updated_readme.splitlines(),\n",
    "    )\n",
    "    lines_added = 0\n",
    "    lines_removed = 0\n",
    "    for line in diff:\n",
    "        if line.startswith('+') and not line.startswith('+++'):\n",
    "            lines_added += 1\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            lines_removed += 1\n",
    "    return ReadmeDiff(lines_added, lines_removed)\n",
    "\n",
    "# Test\n",
    "\n",
    "assert diff_readmes(\"Hello\\nWorld\", \"Hello\\nWorld\\nGoodbye\")\n",
    "\n",
    "\n",
    "from typing import Iterable, Hashable, List\n",
    "\n",
    "\n",
    "def iterate_ngrams(tokens: List[Hashable], n: int) -> Iterable[tuple]:\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i : i + n])\n",
    "\n",
    "\n",
    "def test_iterate_ngrams():\n",
    "    assert list(iterate_ngrams([\"a\", \"b\", \"c\", \"d\"], 2)) == [\n",
    "        (\"a\", \"b\"),\n",
    "        (\"b\", \"c\"),\n",
    "        (\"c\", \"d\"),\n",
    "    ]\n",
    "\n",
    "import re\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # NOTE: This is a very, very basic tokenizer for very basic tasks.\n",
    "    return re.split(r\"\\W+\", text)\n",
    "\n",
    "\n",
    "def test_tokenize():\n",
    "    assert tokenize(\"a b c\") == [\"a\", \"b\", \"c\"]\n",
    "    assert tokenize(\"a, b, c\") == [\"a\", \"b\", \"c\"]\n",
    "\n",
    "\n",
    "def jaccard_similarity(text_a: str, text_b: str, n: int = 4):\n",
    "    summary_ngrams = set(iterate_ngrams(tokenize(text_a), n))\n",
    "    source_ngrams = set(iterate_ngrams(tokenize(text_b), n))\n",
    "\n",
    "    intersection_size = len(summary_ngrams & source_ngrams)\n",
    "    union_size = len(summary_ngrams | source_ngrams)\n",
    "    \n",
    "    return intersection_size / union_size if union_size != 0 else 0\n",
    "\n",
    "\n",
    "example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "example_summary = \"a b c d e f g h i j k\"\n",
    "\n",
    "\n",
    "jaccard_similarity(example_summary, example_source), jaccard_similarity(example_summary, example_source, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_suite = {\n",
    "    'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "    'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    'ktrnka/company-detective': [14, 12, 9, 7, 5],\n",
    "}\n",
    "\n",
    "small_test_suite = {'ktrnka/company-detective': [14, 12, 9, 7, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ktrnka/company-detective...\n",
      "\tTesting PR #14...\n",
      "\tTesting PR #12...\n",
      "\tTesting PR #9...\n",
      "\tTesting PR #7...\n",
      "\tTesting PR #5...\n",
      "\n",
      "Tested against 5 PRs in 1 repos.\n",
      "\n",
      "0% failed.\n",
      "Total runtime: 73s\n",
      "Mean runtime per PR: 15s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional, Tuple\n",
    "from main import ReadmeRecommendation, review_pull_request\n",
    "from time import time\n",
    "\n",
    "class SingleOutcome(NamedTuple):\n",
    "    result: Optional[ReadmeRecommendation]\n",
    "    error: Optional[ValueError]\n",
    "    seconds: float\n",
    "    diff: Optional[Tuple]\n",
    "    similarity: Optional[float]\n",
    "    base_readme: Optional[str] = None\n",
    "\n",
    "test_suite = small_test_suite\n",
    "\n",
    "outcomes = {}\n",
    "for repo_name, pr_numbers in test_suite.items():\n",
    "    print(f\"Testing {repo_name}...\")\n",
    "    for pr_number in pr_numbers:\n",
    "        print(f\"\\tTesting PR #{pr_number}...\")\n",
    "\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        try:\n",
    "            repo = github_client.get_repo(repo_name)\n",
    "            pr = repo.get_pull(pr_number)\n",
    "\n",
    "            # Get the base README\n",
    "            base_readme = repo.get_contents(\"README.md\", ref=pr.base.sha).decoded_content.decode()\n",
    "\n",
    "            result = review_pull_request(repo, pr, use_base_readme=True)\n",
    "\n",
    "            diff_results = None\n",
    "            similarity = None\n",
    "            if result.should_update:\n",
    "                diff_results = diff_readmes(base_readme, result.updated_readme)\n",
    "                similarity = jaccard_similarity(result.updated_readme, base_readme)\n",
    "\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(result, None, time() - start_time, diff_results, similarity, base_readme)\n",
    "        except ValueError as e:\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(None, e, time() - start_time, None, None, base_readme)\n",
    "\n",
    "# summarize the results\n",
    "percent_failed = len([outcome for outcome in outcomes.values() if outcome.result is None]) / len(outcomes)\n",
    "total_runtime = sum(outcome.seconds for outcome in outcomes.values())\n",
    "mean_runtime = total_runtime / len(outcomes)\n",
    "\n",
    "print(f\"\"\"\n",
    "Tested against {len(outcomes)} PRs in {len(test_suite)} repos.\n",
    "\n",
    "{percent_failed:.0%} failed.\n",
    "Total runtime: {total_runtime:.0f}s\n",
    "Mean runtime per PR: {mean_runtime:.0f}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review errors\n",
    "errors = [outcome for outcome in outcomes.values() if outcome.error is not None]\n",
    "\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.error:\n",
    "        print(f\"{outcome_id}: {outcome.result} in {outcome.seconds:.0f}s\")\n",
    "        errors = outcome.error.errors()\n",
    "        for error in errors:\n",
    "            print(f\"\\tError: {error['msg']}\")\n",
    "        example_error_outcome = outcome\n",
    "        # print(f\"\\tError: {outcome.error.msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic_core._pydantic_core.ValidationError\n",
    "\n",
    "# dir(outcome.error)\n",
    "# outcome.error.errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0% of PRs with should_update had no changes.\n",
      "    Lines changed: [10, 12, 18, 25] (avg: 16.2)\n",
      "Percent of READMEs that were lengthened: 100%\n",
      "Mean similarity: 64.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "should_updates = [outcome for outcome in outcomes.values() if outcome.result is not None and outcome.result.should_update]\n",
    "\n",
    "lines_changed = [sum(outcome.diff) for outcome in should_updates]\n",
    "similarities = [outcome.similarity for outcome in should_updates]\n",
    "\n",
    "percent_bad_diff = len([outcome for outcome in should_updates if outcome.diff == (0, 0)]) / len(should_updates)\n",
    "mean_similarity = sum(similarities) / len(similarities)\n",
    "mean_changes = sum(lines_changed) / len(lines_changed)\n",
    "\n",
    "percent_lengthened = len([outcome for outcome in should_updates if outcome.diff[0] > outcome.diff[1]]) / len(should_updates)\n",
    "\n",
    "print(f\"\"\"\n",
    "{percent_bad_diff:.0%} of PRs with should_update had no changes.\n",
    "    Lines changed: {lines_changed} (avg: {mean_changes:.1f})\n",
    "Percent of READMEs that were lengthened: {percent_lengthened:.0%}\n",
    "Mean similarity: {mean_similarity:.1%}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ('ktrnka/company-detective', 14)\n",
      "Automated review took 14s\n",
      "Should update? True\n",
      "Reason: The pull request introduces several changes that should be reflected in the README, including updates to API key handling, additional data sources, and testing information. These updates will provide users with more accurate and comprehensive information about the project.\n",
      "\n",
      "\n",
      "Diff: +8, -2 ✅\n",
      "Jaccard coef of 4grams: 76.9% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/company-detective', 12)\n",
      "Automated review took 17s\n",
      "Should update? True\n",
      "Reason: The pull request introduces significant changes to the project, including the addition of Crunchbase as a new data source and updates to the Glassdoor scraping functionality. These changes should be reflected in the README to provide users with up-to-date information about the project's capabilities and dependencies.\n",
      "\n",
      "\n",
      "Diff: +10, -2 ✅\n",
      "Jaccard coef of 4grams: 77.4% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/company-detective', 9)\n",
      "Automated review took 19s\n",
      "Should update? True\n",
      "Reason: The pull request introduces a significant new feature (dynamic rebuild cadence) and makes several improvements to the project that should be reflected in the README. Additionally, there are some minor updates and clarifications that would improve the overall quality and accuracy of the README.\n",
      "\n",
      "\n",
      "Diff: +16, -2 ✅\n",
      "Jaccard coef of 4grams: 61.7% ❌\n",
      "\n",
      "\n",
      "# ('ktrnka/company-detective', 7)\n",
      "Automated review took 6s\n",
      "Should update? False\n",
      "Reason: The current README is comprehensive and up-to-date. The pull request changes are primarily internal code refactoring and do not introduce any new features or major changes that would require updating the README. The existing README already covers the project overview, features, prerequisites, API key requirements, installation steps, usage instructions, and other relevant information. The changes in the pull request, which involve moving the generic article scraping into utils, do not impact the user-facing aspects of the project described in the README.\n",
      "\n",
      "\n",
      "# ('ktrnka/company-detective', 5)\n",
      "Automated review took 17s\n",
      "Should update? True\n",
      "Reason: The pull request introduces significant changes to the project structure and file organization. The README should be updated to reflect these changes, particularly in the installation instructions, usage section, and project overview.\n",
      "\n",
      "\n",
      "Diff: +20, -5 ✅\n",
      "Jaccard coef of 4grams: 41.0% ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "good_icon = \"✅\"\n",
    "bad_icon = \"❌\"\n",
    "\n",
    "\n",
    "# Review non-errors\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.result:\n",
    "        print(f\"\"\"\n",
    "# {outcome_id}\n",
    "Automated review took {outcome.seconds:.0f}s\n",
    "Should update? {outcome.result.should_update}\n",
    "Reason: {outcome.result.reason}\n",
    "\"\"\")\n",
    "        if outcome.result.should_update:\n",
    "            diff_review = good_icon if sum(outcome.diff) > 0 else bad_icon\n",
    "            similarity_review = good_icon if 0.75 < outcome.similarity < 1. else bad_icon\n",
    "            print(f\"\"\"\n",
    "Diff: +{outcome.diff.added}, -{outcome.diff.removed} {diff_review}\n",
    "Jaccard coef of 4grams: {outcome.similarity:.1%} {similarity_review}\n",
    "\"\"\")\n",
    "\n",
    "            # if sum(outcome.diff) > 0:\n",
    "            #     print(f\"\"\"Updated README: \\n{outcome.result.updated_readme}\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('ktrnka/company-detective', 14), ('ktrnka/company-detective', 12), ('ktrnka/company-detective', 9), ('ktrnka/company-detective', 7), ('ktrnka/company-detective', 5)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- \n",
      "\n",
      "+++ \n",
      "\n",
      "@@ -6,7 +6,7 @@\n",
      "\n",
      " \n",
      " ## Features\n",
      " \n",
      "-- Aggregates information from multiple sources including Crunchbase, Glassdoor, news articles, and company websites\n",
      "+- Aggregates information from multiple sources including Crunchbase, Glassdoor, news articles, company websites, and Reddit\n",
      " - Utilizes AI to summarize and analyze data\n",
      " - Provides a unified summary of company information\n",
      " - Dynamic rebuild cadence for up-to-date information\n",
      "@@ -27,8 +27,9 @@\n",
      "\n",
      " - AWS\n",
      " - Langsmith (Optional)\n",
      " - Crunchbase (via Scrapfly)\n",
      "+- Airtable\n",
      " \n",
      "-Ensure you have obtained the necessary API keys before proceeding with the setup.\n",
      "+Ensure you have obtained the necessary API keys before proceeding with the setup. The project is designed to handle missing API keys gracefully, but functionality may be limited without them.\n",
      " \n",
      " ## Installation\n",
      " \n",
      "@@ -62,10 +63,15 @@\n",
      "\n",
      " - Glassdoor: Offers employee reviews and sentiment analysis.\n",
      " - News Articles: Gathers recent news about the company.\n",
      " - Company Website: Extracts information directly from the company's official website.\n",
      "+- Reddit: Collects relevant discussions and mentions of the company.\n",
      " \n",
      " ## Contributing\n",
      " \n",
      " Contributions are welcome! Please contact Keith for more information on how to contribute, as the repository isn't currently set up for open contributions.\n",
      "+\n",
      "+## Testing\n",
      "+\n",
      "+The project includes automated tests. Note that some network-based tests may be skipped to avoid dependencies on external services during CI/CD processes.\n",
      " \n",
      " ## License\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# show one as a diff\n",
    "example = ('ktrnka/company-detective', 14)\n",
    "\n",
    "outcome = outcomes[example]\n",
    "\n",
    "import difflib\n",
    "diff = difflib.unified_diff(\n",
    "    outcome.base_readme.splitlines(),\n",
    "    outcome.result.updated_readme.splitlines(),\n",
    ")\n",
    "for line in diff:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday afternoon\n",
    "\n",
    "I learned the hard way that something about the ChatPromptTemplate was disabling the Python variables in the Human step,\n",
    "so it was generating the readme purely from guidelines\n",
    "\n",
    "## Checking for \"addition bias\"\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "        Lines changed: [13, 90, 18, 10, 5, 15, 3, 6, 10, 6]\n",
    "    Percent of READMEs that were lengthened: 100%\n",
    "    Mean similarity: 82.6%\n",
    "\n",
    "So 100% of the suggestions here were additions\n",
    "\n",
    "## Trying Jaccard coef\n",
    "\n",
    "Using Jaccard coefficient to assess the extensiveness of the change is useful BUT I have to get used to it and calibrate towards a \"healthy\" range.\n",
    "\n",
    "Updated summary stats (Haiku):\n",
    "\n",
    "    9% of PRs with should_update had no changes.\n",
    "        Lines changed: [15, 76, 87, 14, 9, 0, 8, 3, 19, 73, 50]\n",
    "    Mean similarity: 68.4%\n",
    "\n",
    "Same thing but with Sonnet:\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "        Lines changed: [13, 90, 18, 10, 5, 15, 3, 6, 10, 6]\n",
    "    Mean similarity: 82.6%\n",
    "\n",
    "Ok that's a good sign.\n",
    "\n",
    "## Trying to see if Sonnet fixes some of the quality issues\n",
    "\n",
    "Baseline (Haiku):\n",
    "\n",
    "    20% of PRs with should_update had no changes.\n",
    "    40% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "Sonnet:\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 253s\n",
    "    Mean runtime per PR: 23s (slightly faster than the original non-cached results)\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "    9% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "The one bad case looked like it did a more extensive README rewrite\n",
    "\n",
    "\n",
    "## Increased the max_tokens to 4096 (Haiku's max)\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 105s\n",
    "    Mean runtime per PR: 10s\n",
    "\n",
    "Run 2:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 94s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Ok that did it. Now I'll move on to the next wave of issues:\n",
    "\n",
    "- should_update = True but it outputs the exact README as the input\n",
    "\n",
    "## 1 try only, and lowering temperature\n",
    "\n",
    "Temp 0\n",
    "\n",
    "    55% failed.\n",
    "    Total runtime: 97s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Oof\n",
    "\n",
    "Temp 0.3, which some summarization folks like:\n",
    "\n",
    "    45% failed.\n",
    "    Total runtime: 102s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Eh, I'm going to dial back the summarization stuff\n",
    "\n",
    "## After the fix:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    36% failed.\n",
    "    Total runtime: 91s\n",
    "    Mean runtime per PR: 8s\n",
    "\n",
    "The failed cases are doing should_update=True and updated_readme = nothing. I saw in the raw output that one of the really bad subtractions just had [rest of readme the same] or some such at the bottom.\n",
    "\n",
    "I've done some re-runs and it tends to be 27%-36% error rate which is worse than before (18% just before prompt caching)\n",
    "\n",
    "I attempted to manually specify the input variables but they're completely ignored. I also tried wrapping the human message in an array like the system message but that also failed.\n",
    "\n",
    "After some more experiments I think I figured out why the results are different than before:\n",
    "- Previously I defaulted tries_remaining to 1 but that actually meant it could try and retry\n",
    "- I set that to 1, and re-ran twice and got 27% failure and 18%, so I think that brings it closer to previous results\n",
    "\n",
    "Also I added more instrumentation on the output. This makes it easy to catch cases where it generates a README unrelated to the input repo. It also led me to find many cases in which it said should_update=True and regenerated the EXACT input README which is another red-flag to consider\n",
    "\n",
    "# Dev log: Monday\n",
    "\n",
    "Refactor:\n",
    "- More controlled experiment (multiple repos, build a fixed set of PRs ahead of time)\n",
    "- Track failure rate and execution time\n",
    "- Hold onto any objects to adhoc analysis after running\n",
    "\n",
    "Test suites:\n",
    "\n",
    "    medium_test_suite = {\n",
    "        'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "        'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    }\n",
    "\n",
    "    small_test_suite = {\n",
    "        'ktrnka/update-your-readme': [41, 40],\n",
    "    }\n",
    "\n",
    "## Medium test suite\n",
    "\n",
    "### Baseline with Haiku, before removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 135s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "### After removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 129s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "It's slightly faster but not a lot. I'll keep the change though.\n",
    "\n",
    "### Adding prompt caching\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 54s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Notes\n",
    "- It throws an annoying warning \"extra_headers was transferred to model_kwargs\" but that's what the docs show: https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html\n",
    "- The speedup is wonderful! That's what I'd hoped for\n",
    "- The 0% failure rate is surprising. It's possible that it's a result of needing to refactor to use the SystemMessage vs HumanMessage\n",
    "\n",
    "I'm going to re-run this without any changes cause I kind of don't even believe that we have no errors now:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 53s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Huh\n",
    "\n",
    "## Small test suite\n",
    "\n",
    "### Baseline test\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    50% failed.\n",
    "    Total runtime: 55s\n",
    "    Mean runtime per PR: 28s\n",
    "\n",
    "### With Claude 3 Haiku\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 22s\n",
    "    Mean runtime per PR: 11s\n",
    "\n",
    "\n",
    "\n",
    "# Dev log: Sunday\n",
    "\n",
    "## Before prompt engineering, running on Locust\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Stronger guidance in the prompt itself, like the Pydantic field descriptions and how they're mentioned in the prompt itself\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 2, 'should_update': 1, 'no_update': 1})\n",
    "\n",
    "## Retries\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Prompt updates, Pydantic model updates\n",
    "Counter({'should_update': 3, 'ValueError': 1})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-tOcPalp-",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
