{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keith/.local/share/virtualenvs/update-your-readme-tOcPalp-/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from core import github_client\n",
    "\n",
    "def build_evaluation_suite(repo_full_name: str, state: str = 'open', limit: int = 10):\n",
    "    print(f\"Building evaluation suite for {repo_full_name} with PRs in state {state}\")\n",
    "    repo = github_client.get_repo(repo_full_name)\n",
    "\n",
    "    pr_numbers = []\n",
    "    \n",
    "    for i, pr in enumerate(repo.get_pulls(state=state)):\n",
    "        if pr.user.login == \"github-actions[bot]\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tPR #{pr.number}, {pr.user.login}: {pr.title}\")\n",
    "        pr_numbers.append(pr.number)\n",
    "\n",
    "        if i >= limit:\n",
    "            print(f\"\\tReached limit of {limit} PRs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\tExample config: {}\\n\".format({\n",
    "        repo_full_name: pr_numbers\n",
    "    }))\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "# build_evaluation_suite(\"locustio/locust\")\n",
    "# build_evaluation_suite(\"pandas-dev/pandas\")\n",
    "# build_evaluation_suite(\"ktrnka/update-your-readme\", state='closed', limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from typing import NamedTuple\n",
    "\n",
    "class ReadmeDiff(NamedTuple):\n",
    "    added: int\n",
    "    removed: int\n",
    "\n",
    "def diff_readmes(original_readme: str, updated_readme: str):\n",
    "    diff = difflib.unified_diff(\n",
    "        original_readme.splitlines(),\n",
    "        updated_readme.splitlines(),\n",
    "    )\n",
    "    lines_added = 0\n",
    "    lines_removed = 0\n",
    "    for line in diff:\n",
    "        if line.startswith('+') and not line.startswith('+++'):\n",
    "            lines_added += 1\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            lines_removed += 1\n",
    "    return ReadmeDiff(lines_added, lines_removed)\n",
    "\n",
    "# Test\n",
    "\n",
    "assert diff_readmes(\"Hello\\nWorld\", \"Hello\\nWorld\\nGoodbye\")\n",
    "\n",
    "\n",
    "from typing import Iterable, Hashable, List\n",
    "\n",
    "\n",
    "def iterate_ngrams(tokens: List[Hashable], n: int) -> Iterable[tuple]:\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i : i + n])\n",
    "\n",
    "\n",
    "def test_iterate_ngrams():\n",
    "    assert list(iterate_ngrams([\"a\", \"b\", \"c\", \"d\"], 2)) == [\n",
    "        (\"a\", \"b\"),\n",
    "        (\"b\", \"c\"),\n",
    "        (\"c\", \"d\"),\n",
    "    ]\n",
    "\n",
    "import re\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # NOTE: This is a very, very basic tokenizer for very basic tasks.\n",
    "    return re.split(r\"\\W+\", text)\n",
    "\n",
    "\n",
    "def test_tokenize():\n",
    "    assert tokenize(\"a b c\") == [\"a\", \"b\", \"c\"]\n",
    "    assert tokenize(\"a, b, c\") == [\"a\", \"b\", \"c\"]\n",
    "\n",
    "\n",
    "def extractive_fraction(summary: str, source: str, n: int = 4):\n",
    "    summary_ngrams = set(iterate_ngrams(tokenize(summary), n))\n",
    "    source_ngrams = set(iterate_ngrams(tokenize(source), n))\n",
    "    return len(summary_ngrams & source_ngrams) / len(summary_ngrams)\n",
    "\n",
    "\n",
    "def test_extractive_fraction():\n",
    "    example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "    example_summary = \"a b c d e f g h i j k\"\n",
    "\n",
    "    assert extractive_fraction(example_summary, example_source) == 1.0\n",
    "\n",
    "    example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "    example_summary = \"a b c d e g h i j k\"\n",
    "\n",
    "    assert extractive_fraction(example_summary, example_source) < 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_suite = {\n",
    "    'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "    'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "}\n",
    "\n",
    "small_test_suite = {\n",
    "    'ktrnka/update-your-readme': [41, 40],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing locustio/locust...\n",
      "\tTesting PR #2899...\n",
      "\tTesting PR #2856...\n",
      "\tTesting PR #2820...\n",
      "\tTesting PR #2786...\n",
      "Testing ktrnka/update-your-readme...\n",
      "\tTesting PR #50...\n",
      "\tTesting PR #49...\n",
      "\tTesting PR #46...\n",
      "\tTesting PR #44...\n",
      "\tTesting PR #43...\n",
      "\tTesting PR #41...\n",
      "\tTesting PR #40...\n",
      "\n",
      "Tested against 11 PRs in 2 repos.\n",
      "\n",
      "0% failed.\n",
      "Total runtime: 253s\n",
      "Mean runtime per PR: 23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional, Tuple\n",
    "from core import ReadmeRecommendation, review_pull_request\n",
    "from time import time\n",
    "\n",
    "class SingleOutcome(NamedTuple):\n",
    "    result: Optional[ReadmeRecommendation]\n",
    "    error: Optional[ValueError]\n",
    "    seconds: float\n",
    "    diff: Optional[Tuple]\n",
    "    extractive_ngram_fraction: Optional[float]\n",
    "\n",
    "test_suite = medium_test_suite\n",
    "\n",
    "outcomes = {}\n",
    "for repo_name, pr_numbers in test_suite.items():\n",
    "    print(f\"Testing {repo_name}...\")\n",
    "    for pr_number in pr_numbers:\n",
    "        print(f\"\\tTesting PR #{pr_number}...\")\n",
    "\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        try:\n",
    "            repo = github_client.get_repo(repo_name)\n",
    "            pr = repo.get_pull(pr_number)\n",
    "\n",
    "            # Get the base README\n",
    "            base_readme = repo.get_contents(\"README.md\", ref=pr.base.sha).decoded_content.decode()\n",
    "\n",
    "            result = review_pull_request(repo, pr)\n",
    "\n",
    "            diff_results = None\n",
    "            extractive_ngram_fraction = None\n",
    "            if result.should_update:\n",
    "                diff_results = diff_readmes(base_readme, result.updated_readme)\n",
    "                extractive_ngram_fraction = extractive_fraction(result.updated_readme, base_readme)\n",
    "\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(result, None, time() - start_time, diff_results, extractive_ngram_fraction)\n",
    "        except ValueError as e:\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(None, e, time() - start_time, None, None)\n",
    "\n",
    "# summarize the results\n",
    "percent_failed = len([outcome for outcome in outcomes.values() if outcome.result is None]) / len(outcomes)\n",
    "total_runtime = sum(outcome.seconds for outcome in outcomes.values())\n",
    "mean_runtime = total_runtime / len(outcomes)\n",
    "\n",
    "print(f\"\"\"\n",
    "Tested against {len(outcomes)} PRs in {len(test_suite)} repos.\n",
    "\n",
    "{percent_failed:.0%} failed.\n",
    "Total runtime: {total_runtime:.0f}s\n",
    "Mean runtime per PR: {mean_runtime:.0f}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review errors\n",
    "errors = [outcome for outcome in outcomes.values() if outcome.error is not None]\n",
    "\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.error:\n",
    "        print(f\"{outcome_id}: {outcome.result} in {outcome.seconds:.0f}s\")\n",
    "        errors = outcome.error.errors()\n",
    "        for error in errors:\n",
    "            print(f\"\\tError: {error['msg']}\")\n",
    "        example_error_outcome = outcome\n",
    "        # print(f\"\\tError: {outcome.error.msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic_core._pydantic_core.ValidationError\n",
    "\n",
    "# dir(outcome.error)\n",
    "# outcome.error.errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0% of PRs with should_update had no changes.\n",
      "9% of PRs with should_update had bad extractive ngram percent.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "should_updates = [outcome for outcome in outcomes.values() if outcome.result is not None and outcome.result.should_update]\n",
    "\n",
    "percent_bad_diff = len([outcome for outcome in should_updates if sum(outcome.diff) == 0]) / len(should_updates)\n",
    "percent_bad_extractive = len([outcome for outcome in should_updates if outcome.extractive_ngram_fraction < 0.75 or outcome.extractive_ngram_fraction  == 1]) / len(should_updates)\n",
    "\n",
    "print(f\"\"\"\n",
    "{percent_bad_diff:.0%} of PRs with should_update had no changes.\n",
    "{percent_bad_extractive:.0%} of PRs with should_update had bad extractive ngram percent.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ('locustio/locust', 2899)\n",
      "Automated review took 36s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces significant changes to the testing infrastructure, particularly the implementation of polling mechanisms to improve test reliability. This is an important development that should be reflected in the README to highlight Locust's commitment to robust testing and continuous improvement.\n",
      "Diff: +5, -1 ✅\n",
      "Extractive n-gram fraction: 94.1% ✅\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2856)\n",
      "Automated review took 21s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to improve clarity, add more detailed information about features, and include guidance on getting started. The current README is good but can be enhanced to provide a better overview of Locust's capabilities and usage.\n",
      "Diff: +51, -47 ✅\n",
      "Extractive n-gram fraction: 45.5% ❌\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2820)\n",
      "Automated review took 35s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to include information about the recent changes in handling task weights efficiently in TaskSets. This is a significant improvement that affects how tasks are defined and executed, which is important for users of the library.\n",
      "Diff: +5, -1 ✅\n",
      "Extractive n-gram fraction: 93.4% ✅\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2786)\n",
      "Automated review took 36s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to reflect the new CLI argument for selecting UserClasses to run. This change is significant as it affects how users can specify which User classes to run in their Locust tests. The current README doesn't mention this new feature, and the example command line usage is now outdated.\n",
      "Diff: +23, -1 ✅\n",
      "Extractive n-gram fraction: 89.6% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 50)\n",
      "Automated review took 16s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces changes to the project structure and workflow files that should be reflected in the README. Specifically, it removes the main.py file, adds a new workflow file (readme_feedback.yml), and slightly changes the project structure. These changes should be documented in the README to keep it up-to-date and accurate.\n",
      "Diff: +7, -3 ✅\n",
      "Extractive n-gram fraction: 93.4% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 49)\n",
      "Automated review took 15s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new error handling mechanism in the `repo_contents_to_markdown` function, which is a part of the core functionality. This change should be reflected in the README to inform users about the improved robustness of the project.\n",
      "Diff: +5, -0 ✅\n",
      "Extractive n-gram fraction: 91.6% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 46)\n",
      "Automated review took 22s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces significant changes to the GitHub Actions workflow and the action.yml file. These changes affect the usage and setup of the project, which should be reflected in the README for better user understanding and easier adoption.\n",
      "Diff: +24, -1 ✅\n",
      "Extractive n-gram fraction: 77.2% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 44)\n",
      "Automated review took 15s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request adds a new file `main.py` to the project structure, which should be reflected in the README. Additionally, we can improve the README by adding information about this new file and its purpose.\n",
      "Diff: +9, -1 ✅\n",
      "Extractive n-gram fraction: 87.1% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 43)\n",
      "Automated review took 19s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new feature to supply user feedback to the README update process. This significant change should be reflected in the README to inform users about this new functionality and how to use it.\n",
      "Diff: +6, -0 ✅\n",
      "Extractive n-gram fraction: 87.7% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 41)\n",
      "Automated review took 21s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces changes to improve the reliability of the README update process, which should be reflected in the README. Additionally, the README can be enhanced to provide more details about the error handling and retry mechanism.\n",
      "Diff: +9, -0 ✅\n",
      "Extractive n-gram fraction: 83.6% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 40)\n",
      "Automated review took 17s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request introduces a new feature for user feedback in the README update process. This change should be reflected in the README to inform users about this new functionality and how to use it.\n",
      "Diff: +6, -0 ✅\n",
      "Extractive n-gram fraction: 87.2% ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "good_icon = \"✅\"\n",
    "bad_icon = \"❌\"\n",
    "\n",
    "\n",
    "# Review non-errors\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.result:\n",
    "        print(f\"\"\"\n",
    "# {outcome_id}\n",
    "Automated review took {outcome.seconds:.0f}s\n",
    "Should update? {outcome.result.should_update}\n",
    "\"\"\")\n",
    "        if outcome.result.should_update:\n",
    "            diff_review = good_icon if sum(outcome.diff) > 0 else bad_icon\n",
    "            extractive_review = good_icon if 0.75 < outcome.extractive_ngram_fraction < 1. else bad_icon\n",
    "            print(f\"\"\"Reason: {outcome.result.reason}\n",
    "Diff: +{outcome.diff.added}, -{outcome.diff.removed} {diff_review}\n",
    "Extractive n-gram fraction: {outcome.extractive_ngram_fraction:.1%} {extractive_review}\n",
    "\"\"\")\n",
    "\n",
    "            # if sum(outcome.diff) > 0:\n",
    "            #     print(f\"\"\"Updated README: \\n{outcome.result.updated_readme}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday afternoon\n",
    "\n",
    "I learned the hard way that something about the ChatPromptTemplate was disabling the Python variables in the Human step,\n",
    "so it was generating the readme purely from guidelines\n",
    "\n",
    "## Trying to see if Sonnet fixes some of the quality issues\n",
    "\n",
    "Baseline (Haiku):\n",
    "\n",
    "    20% of PRs with should_update had no changes.\n",
    "    40% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "Sonnet:\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 253s\n",
    "    Mean runtime per PR: 23s (slightly faster than the original non-cached results)\n",
    "\n",
    "    0% of PRs with should_update had no changes.\n",
    "    9% of PRs with should_update had bad extractive ngram percent.\n",
    "\n",
    "The one bad case looked like it did a more extensive README rewrite\n",
    "\n",
    "\n",
    "## Increased the max_tokens to 4096 (Haiku's max)\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 105s\n",
    "    Mean runtime per PR: 10s\n",
    "\n",
    "Run 2:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 94s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Ok that did it. Now I'll move on to the next wave of issues:\n",
    "\n",
    "- should_update = True but it outputs the exact README as the input\n",
    "\n",
    "## 1 try only, and lowering temperature\n",
    "\n",
    "Temp 0\n",
    "\n",
    "    55% failed.\n",
    "    Total runtime: 97s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Oof\n",
    "\n",
    "Temp 0.3, which some summarization folks like:\n",
    "\n",
    "    45% failed.\n",
    "    Total runtime: 102s\n",
    "    Mean runtime per PR: 9s\n",
    "\n",
    "Eh, I'm going to dial back the summarization stuff\n",
    "\n",
    "## After the fix:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    36% failed.\n",
    "    Total runtime: 91s\n",
    "    Mean runtime per PR: 8s\n",
    "\n",
    "The failed cases are doing should_update=True and updated_readme = nothing. I saw in the raw output that one of the really bad subtractions just had [rest of readme the same] or some such at the bottom.\n",
    "\n",
    "I've done some re-runs and it tends to be 27%-36% error rate which is worse than before (18% just before prompt caching)\n",
    "\n",
    "I attempted to manually specify the input variables but they're completely ignored. I also tried wrapping the human message in an array like the system message but that also failed.\n",
    "\n",
    "After some more experiments I think I figured out why the results are different than before:\n",
    "- Previously I defaulted tries_remaining to 1 but that actually meant it could try and retry\n",
    "- I set that to 1, and re-ran twice and got 27% failure and 18%, so I think that brings it closer to previous results\n",
    "\n",
    "Also I added more instrumentation on the output. This makes it easy to catch cases where it generates a README unrelated to the input repo. It also led me to find many cases in which it said should_update=True and regenerated the EXACT input README which is another red-flag to consider\n",
    "\n",
    "# Dev log: Monday\n",
    "\n",
    "Refactor:\n",
    "- More controlled experiment (multiple repos, build a fixed set of PRs ahead of time)\n",
    "- Track failure rate and execution time\n",
    "- Hold onto any objects to adhoc analysis after running\n",
    "\n",
    "Test suites:\n",
    "\n",
    "    medium_test_suite = {\n",
    "        'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "        'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    }\n",
    "\n",
    "    small_test_suite = {\n",
    "        'ktrnka/update-your-readme': [41, 40],\n",
    "    }\n",
    "\n",
    "## Medium test suite\n",
    "\n",
    "### Baseline with Haiku, before removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 135s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "### After removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 129s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "It's slightly faster but not a lot. I'll keep the change though.\n",
    "\n",
    "### Adding prompt caching\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 54s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Notes\n",
    "- It throws an annoying warning \"extra_headers was transferred to model_kwargs\" but that's what the docs show: https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html\n",
    "- The speedup is wonderful! That's what I'd hoped for\n",
    "- The 0% failure rate is surprising. It's possible that it's a result of needing to refactor to use the SystemMessage vs HumanMessage\n",
    "\n",
    "I'm going to re-run this without any changes cause I kind of don't even believe that we have no errors now:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 53s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Huh\n",
    "\n",
    "## Small test suite\n",
    "\n",
    "### Baseline test\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    50% failed.\n",
    "    Total runtime: 55s\n",
    "    Mean runtime per PR: 28s\n",
    "\n",
    "### With Claude 3 Haiku\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 22s\n",
    "    Mean runtime per PR: 11s\n",
    "\n",
    "\n",
    "\n",
    "# Dev log: Sunday\n",
    "\n",
    "## Before prompt engineering, running on Locust\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Stronger guidance in the prompt itself, like the Pydantic field descriptions and how they're mentioned in the prompt itself\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 2, 'should_update': 1, 'no_update': 1})\n",
    "\n",
    "## Retries\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Prompt updates, Pydantic model updates\n",
    "Counter({'should_update': 3, 'ValueError': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-LRwUm5rQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
