{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import github_client\n",
    "\n",
    "def build_evaluation_suite(repo_full_name: str, state: str = 'open', limit: int = 10):\n",
    "    print(f\"Building evaluation suite for {repo_full_name} with PRs in state {state}\")\n",
    "    repo = github_client.get_repo(repo_full_name)\n",
    "\n",
    "    pr_numbers = []\n",
    "    \n",
    "    for i, pr in enumerate(repo.get_pulls(state=state)):\n",
    "        if pr.user.login == \"github-actions[bot]\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\tPR #{pr.number}, {pr.user.login}: {pr.title}\")\n",
    "        pr_numbers.append(pr.number)\n",
    "\n",
    "        if i >= limit:\n",
    "            print(f\"\\tReached limit of {limit} PRs\")\n",
    "            break\n",
    "\n",
    "    print(\"\\tExample config: {}\\n\".format({\n",
    "        repo_full_name: pr_numbers\n",
    "    }))\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "# build_evaluation_suite(\"locustio/locust\")\n",
    "# build_evaluation_suite(\"pandas-dev/pandas\")\n",
    "# build_evaluation_suite(\"ktrnka/update-your-readme\", state='closed', limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from typing import NamedTuple\n",
    "\n",
    "class ReadmeDiff(NamedTuple):\n",
    "    added: int\n",
    "    removed: int\n",
    "\n",
    "def diff_readmes(original_readme: str, updated_readme: str):\n",
    "    diff = difflib.unified_diff(\n",
    "        original_readme.splitlines(),\n",
    "        updated_readme.splitlines(),\n",
    "    )\n",
    "    lines_added = 0\n",
    "    lines_removed = 0\n",
    "    for line in diff:\n",
    "        if line.startswith('+') and not line.startswith('+++'):\n",
    "            lines_added += 1\n",
    "        elif line.startswith('-') and not line.startswith('---'):\n",
    "            lines_removed += 1\n",
    "    return ReadmeDiff(lines_added, lines_removed)\n",
    "\n",
    "# Test\n",
    "\n",
    "assert diff_readmes(\"Hello\\nWorld\", \"Hello\\nWorld\\nGoodbye\")\n",
    "\n",
    "\n",
    "from typing import Iterable, Hashable, List\n",
    "\n",
    "\n",
    "def iterate_ngrams(tokens: List[Hashable], n: int) -> Iterable[tuple]:\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        yield tuple(tokens[i : i + n])\n",
    "\n",
    "\n",
    "def test_iterate_ngrams():\n",
    "    assert list(iterate_ngrams([\"a\", \"b\", \"c\", \"d\"], 2)) == [\n",
    "        (\"a\", \"b\"),\n",
    "        (\"b\", \"c\"),\n",
    "        (\"c\", \"d\"),\n",
    "    ]\n",
    "\n",
    "import re\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    # NOTE: This is a very, very basic tokenizer for very basic tasks.\n",
    "    return re.split(r\"\\W+\", text)\n",
    "\n",
    "\n",
    "def test_tokenize():\n",
    "    assert tokenize(\"a b c\") == [\"a\", \"b\", \"c\"]\n",
    "    assert tokenize(\"a, b, c\") == [\"a\", \"b\", \"c\"]\n",
    "\n",
    "\n",
    "def extractive_fraction(summary: str, source: str, n: int = 4):\n",
    "    summary_ngrams = set(iterate_ngrams(tokenize(summary), n))\n",
    "    source_ngrams = set(iterate_ngrams(tokenize(source), n))\n",
    "    return len(summary_ngrams & source_ngrams) / len(summary_ngrams)\n",
    "\n",
    "\n",
    "def test_extractive_fraction():\n",
    "    example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "    example_summary = \"a b c d e f g h i j k\"\n",
    "\n",
    "    assert extractive_fraction(example_summary, example_source) == 1.0\n",
    "\n",
    "    example_source = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
    "    example_summary = \"a b c d e g h i j k\"\n",
    "\n",
    "    assert extractive_fraction(example_summary, example_source) < 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_test_suite = {\n",
    "    'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "    'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "}\n",
    "\n",
    "small_test_suite = {\n",
    "    'ktrnka/update-your-readme': [41, 40],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing locustio/locust...\n",
      "\tTesting PR #2899...\n",
      "\tTesting PR #2856...\n",
      "Validation error, trying again\n",
      "\tTesting PR #2820...\n",
      "\tTesting PR #2786...\n",
      "Validation error, trying again\n",
      "Testing ktrnka/update-your-readme...\n",
      "\tTesting PR #50...\n",
      "\tTesting PR #49...\n",
      "\tTesting PR #46...\n",
      "\tTesting PR #44...\n",
      "Validation error, trying again\n",
      "\tTesting PR #43...\n",
      "Validation error, trying again\n",
      "\tTesting PR #41...\n",
      "\tTesting PR #40...\n",
      "\n",
      "Tested against 11 PRs in 2 repos.\n",
      "\n",
      "18% failed.\n",
      "Total runtime: 130s\n",
      "Mean runtime per PR: 12s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Optional, Tuple\n",
    "from core import ReadmeRecommendation, review_pull_request\n",
    "from time import time\n",
    "\n",
    "class SingleOutcome(NamedTuple):\n",
    "    result: Optional[ReadmeRecommendation]\n",
    "    error: Optional[ValueError]\n",
    "    seconds: float\n",
    "    diff: Optional[Tuple]\n",
    "    extractive_ngram_fraction: Optional[float]\n",
    "\n",
    "test_suite = medium_test_suite\n",
    "\n",
    "outcomes = {}\n",
    "for repo_name, pr_numbers in test_suite.items():\n",
    "    print(f\"Testing {repo_name}...\")\n",
    "    for pr_number in pr_numbers:\n",
    "        print(f\"\\tTesting PR #{pr_number}...\")\n",
    "\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        try:\n",
    "            repo = github_client.get_repo(repo_name)\n",
    "            pr = repo.get_pull(pr_number)\n",
    "\n",
    "            # Get the base README\n",
    "            base_readme = repo.get_contents(\"README.md\", ref=pr.base.sha).decoded_content.decode()\n",
    "\n",
    "            result = review_pull_request(repo, pr)\n",
    "\n",
    "            diff_results = None\n",
    "            extractive_ngram_fraction = None\n",
    "            if result.should_update:\n",
    "                diff_results = diff_readmes(base_readme, result.updated_readme)\n",
    "                extractive_ngram_fraction = extractive_fraction(result.updated_readme, base_readme)\n",
    "\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(result, None, time() - start_time, diff_results, extractive_ngram_fraction)\n",
    "        except ValueError as e:\n",
    "            outcomes[(repo_name, pr_number)] = SingleOutcome(None, e, time() - start_time, None, None)\n",
    "\n",
    "# summarize the results\n",
    "percent_failed = len([outcome for outcome in outcomes.values() if outcome.result is None]) / len(outcomes)\n",
    "total_runtime = sum(outcome.seconds for outcome in outcomes.values())\n",
    "mean_runtime = total_runtime / len(outcomes)\n",
    "\n",
    "print(f\"\"\"\n",
    "Tested against {len(outcomes)} PRs in {len(test_suite)} repos.\n",
    "\n",
    "{percent_failed:.0%} failed.\n",
    "Total runtime: {total_runtime:.0f}s\n",
    "Mean runtime per PR: {mean_runtime:.0f}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('locustio/locust', 2856): None in 21s\n",
      "\tError: Value error, updated_readme must be provided if should_update is True\n",
      "('locustio/locust', 2786): None in 23s\n",
      "\tError: Value error, updated_readme must be provided if should_update is True\n"
     ]
    }
   ],
   "source": [
    "# Review errors\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.error:\n",
    "        print(f\"{outcome_id}: {outcome.result} in {outcome.seconds:.0f}s\")\n",
    "        errors = outcome.error.errors()\n",
    "        for error in errors:\n",
    "            print(f\"\\tError: {error['msg']}\")\n",
    "        # print(f\"\\tError: {outcome.error.msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic_core._pydantic_core.ValidationError\n",
    "\n",
    "# dir(outcome.error)\n",
    "# outcome.error.errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ('locustio/locust', 2899)\n",
      "Automated review took 3s\n",
      "Should update? False\n",
      "\n",
      "\n",
      "# ('locustio/locust', 2820)\n",
      "Automated review took 10s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README could be improved to better highlight the key features and capabilities of Locust. The current README provides a good overview, but could be made more engaging and informative.\n",
      "Diff: +22, -69 ✅\n",
      "Extractive n-gram fraction: 35.8% ❌\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 50)\n",
      "Automated review took 9s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request removes the main.py file from the project structure, but the README still references it. The project structure section of the README should be updated to reflect this change.\n",
      "Diff: +6, -3 ✅\n",
      "Extractive n-gram fraction: 94.7% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 49)\n",
      "Automated review took 8s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to address the changes made in the pull request. The contributor has added new content to the core.py file, which is not reflected in the current README. To ensure the documentation remains up-to-date, the README should be updated to include information about the new changes.\n",
      "Diff: +0, -0 ❌\n",
      "Extractive n-gram fraction: 100.0% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 46)\n",
      "Automated review took 10s\n",
      "Should update? True\n",
      "\n",
      "Reason: The existing README could be improved to provide more details and guidance for users. The pull request changes indicate that the project has a good set of features and GitHub Actions integration, which should be clearly documented in the README.\n",
      "Diff: +0, -0 ❌\n",
      "Extractive n-gram fraction: 100.0% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 44)\n",
      "Automated review took 15s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README should be updated to reflect the new 'main.py' file that has been added to the project structure.\n",
      "Diff: +2, -1 ✅\n",
      "Extractive n-gram fraction: 98.9% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 43)\n",
      "Automated review took 15s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request adds a new option to supply user feedback, which is an important feature that should be documented in the README.\n",
      "Diff: +3, -55 ✅\n",
      "Extractive n-gram fraction: 88.7% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 41)\n",
      "Automated review took 9s\n",
      "Should update? True\n",
      "\n",
      "Reason: The README could be improved to provide more detailed installation and usage instructions, as well as better formatting and organization. The current README lacks clear step-by-step installation instructions, and the project structure section could be expanded to better explain the different components of the project.\n",
      "Diff: +25, -29 ✅\n",
      "Extractive n-gram fraction: 63.8% ✅\n",
      "\n",
      "\n",
      "# ('ktrnka/update-your-readme', 40)\n",
      "Automated review took 7s\n",
      "Should update? True\n",
      "\n",
      "Reason: The pull request adds a new feature to allow providing user feedback for the README update process. This feedback should be incorporated into the README to provide more complete documentation for the project.\n",
      "Diff: +3, -55 ✅\n",
      "Extractive n-gram fraction: 89.1% ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "good_icon = \"✅\"\n",
    "bad_icon = \"❌\"\n",
    "\n",
    "# Review non-errors\n",
    "for outcome_id, outcome in outcomes.items():\n",
    "    if outcome.result:\n",
    "        print(f\"\"\"\n",
    "# {outcome_id}\n",
    "Automated review took {outcome.seconds:.0f}s\n",
    "Should update? {outcome.result.should_update}\n",
    "\"\"\")\n",
    "        if outcome.result.should_update:\n",
    "            diff_review = good_icon if sum(outcome.diff) > 0 else bad_icon\n",
    "            extractive_review = good_icon if outcome.extractive_ngram_fraction > 0.5 else bad_icon\n",
    "            print(f\"\"\"Reason: {outcome.result.reason}\n",
    "Diff: +{outcome.diff.added}, -{outcome.diff.removed} {diff_review}\n",
    "Extractive n-gram fraction: {outcome.extractive_ngram_fraction:.1%} {extractive_review}\n",
    "\"\"\")\n",
    "\n",
    "            # if sum(outcome.diff) > 0:\n",
    "            #     print(f\"\"\"Updated README: \\n{outcome.result.updated_readme}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monday afternoon\n",
    "\n",
    "I learned the hard way that something about the ChatPromptTemplate was disabling the Python variables in the Human step,\n",
    "so it was generating the readme purely from guidelines\n",
    "\n",
    "## 1 try only, and lowering temperature\n",
    "\n",
    "\n",
    "## After the fix:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    36% failed.\n",
    "    Total runtime: 91s\n",
    "    Mean runtime per PR: 8s\n",
    "\n",
    "The failed cases are doing should_update=True and updated_readme = nothing. I saw in the raw output that one of the really bad subtractions just had [rest of readme the same] or some such at the bottom.\n",
    "\n",
    "I've done some re-runs and it tends to be 27%-36% error rate which is worse than before (18% just before prompt caching)\n",
    "\n",
    "I attempted to manually specify the input variables but they're completely ignored. I also tried wrapping the human message in an array like the system message but that also failed.\n",
    "\n",
    "After some more experiments I think I figured out why the results are different than before:\n",
    "- Previously I defaulted tries_remaining to 1 but that actually meant it could try and retry\n",
    "- I set that to 1, and re-ran twice and got 27% failure and 18%, so I think that brings it closer to previous results\n",
    "\n",
    "Also I added more instrumentation on the output. This makes it easy to catch cases where it generates a README unrelated to the input repo. It also led me to find many cases in which it said should_update=True and regenerated the EXACT input README which is another red-flag to consider\n",
    "\n",
    "# Dev log: Monday\n",
    "\n",
    "Refactor:\n",
    "- More controlled experiment (multiple repos, build a fixed set of PRs ahead of time)\n",
    "- Track failure rate and execution time\n",
    "- Hold onto any objects to adhoc analysis after running\n",
    "\n",
    "Test suites:\n",
    "\n",
    "    medium_test_suite = {\n",
    "        'locustio/locust': [2899, 2856, 2820, 2786],\n",
    "        'ktrnka/update-your-readme': [50, 49, 46, 44, 43, 41, 40],\n",
    "    }\n",
    "\n",
    "    small_test_suite = {\n",
    "        'ktrnka/update-your-readme': [41, 40],\n",
    "    }\n",
    "\n",
    "## Medium test suite\n",
    "\n",
    "### Baseline with Haiku, before removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 135s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "### After removing the directory tree\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    18% failed.\n",
    "    Total runtime: 129s\n",
    "    Mean runtime per PR: 12s\n",
    "\n",
    "It's slightly faster but not a lot. I'll keep the change though.\n",
    "\n",
    "### Adding prompt caching\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 54s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Notes\n",
    "- It throws an annoying warning \"extra_headers was transferred to model_kwargs\" but that's what the docs show: https://api.python.langchain.com/en/latest/chat_models/langchain_anthropic.chat_models.ChatAnthropic.html\n",
    "- The speedup is wonderful! That's what I'd hoped for\n",
    "- The 0% failure rate is surprising. It's possible that it's a result of needing to refactor to use the SystemMessage vs HumanMessage\n",
    "\n",
    "I'm going to re-run this without any changes cause I kind of don't even believe that we have no errors now:\n",
    "\n",
    "    Tested against 11 PRs in 2 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 53s\n",
    "    Mean runtime per PR: 5s\n",
    "\n",
    "Huh\n",
    "\n",
    "## Small test suite\n",
    "\n",
    "### Baseline test\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    50% failed.\n",
    "    Total runtime: 55s\n",
    "    Mean runtime per PR: 28s\n",
    "\n",
    "### With Claude 3 Haiku\n",
    "\n",
    "    Tested against 2 PRs in 1 repos.\n",
    "\n",
    "    0% failed.\n",
    "    Total runtime: 22s\n",
    "    Mean runtime per PR: 11s\n",
    "\n",
    "\n",
    "\n",
    "# Dev log: Sunday\n",
    "\n",
    "## Before prompt engineering, running on Locust\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Stronger guidance in the prompt itself, like the Pydantic field descriptions and how they're mentioned in the prompt itself\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 4})\n",
    "Counter({'ValidationError': 2, 'should_update': 1, 'no_update': 1})\n",
    "\n",
    "## Retries\n",
    "Counter({'ValidationError': 3, 'should_update': 1})\n",
    "\n",
    "## Prompt updates, Pydantic model updates\n",
    "Counter({'should_update': 3, 'ValueError': 1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "update-your-readme-LRwUm5rQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
